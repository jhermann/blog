
var documents = [{
    "id": 0,
    "url": "https://jhermann.github.io/404.html",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "https://jhermann.github.io/about/",
    "title": "üë§Ô∏è About Me",
    "body": "üìù J√ºrgen Hermann ¬∑ üìß jh@web. de ¬∑ üë®‚Äçüè´ Talks &amp; Presentations ¬∑ üß©¬†My¬†(Software)¬†Projects üêç A long long time ago, I started with Python 1. 5 at web. de,founded the MoinMoin wiki project shortly thereafter, and am nowenjoying the increased traction that Python recently gets fromdata science and machine learning.  This blog is powered by fastpages "
    }, {
    "id": 2,
    "url": "https://jhermann.github.io/categories/",
    "title": "üìÇÔ∏è Categories",
    "body": "Contents: {% if site. categories. size &gt; 0 %} {% for category in site. categories %} {% capture category_name %}{{ category | first }}{% endcapture %} {{ category_name }}{% endfor %}{% endif %} {% for category in site. categories %}  {% capture category_name %}{{ category | first }}{% endcapture %} &lt;h3 id = {{ category_name }} &gt;&lt;i class= fas fa-tags category-tags-icon &gt;&lt;/i&gt;&lt;/i&gt; {{ category_name }}&lt;/h3&gt;&lt;a name= {{ category_name | slugize }} &gt;&lt;/a&gt;{% for post in site. categories[category_name] %}{%- assign date_format = site. minima. date_format | default:  %b %-d, %Y  -%}&lt;article class= archive-item &gt; &lt;p class= post-meta post-meta-title &gt;&lt;a class= page-meta  href= {{ site. baseurl }}{{ post. url }} &gt;{{post. title}}&lt;/a&gt; ‚Ä¢ {{ post. date | date: date_format }}&lt;/p&gt;&lt;/article&gt;{% endfor %} {% endfor %}"
    }, {
    "id": 3,
    "url": "https://jhermann.github.io/images/copied_from_nb/",
    "title": "",
    "body": "WarningDo not manually save images into this folder. This is used by GitHub Actions to automatically copy images.  Any images you save into this folder could be deleted at build time. "
    }, {
    "id": 4,
    "url": "https://jhermann.github.io/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ ‚Äúsitemap. xml‚Äù   absolute_url }}   "
    }, {
    "id": 5,
    "url": "https://jhermann.github.io/python/devops/2020/03/28/dh_virtualenv_howto.html",
    "title": "Packaging Python Applications for Debian",
    "body": "2020/03/28 -            This post shows how to easily deploy any Python application in form of an ‚Äòomnibus‚ÄôDebian package, i. e. one that contains all the application's dependencies, just likein a Java WAR. A basic understanding of Debian packaging, the Linux command prompt,and Python tooling is assumed. Introduction : In this article, I'll show how to use dh-virtualenv to create self-contained Debian packages to deploy a Python application. The resulting package is very similar to a executable JAR that you can start via java -jar, in that it contains all the moving parts except Python itself, without influencing or being influenced by version requirements of other applications. This also frees you from being restricted to the dependencies and their versions found on your target platforms, and makes porting to several different target environments easier. The advantage of using a Debian package for deployment as opposed to the native Python tool chain is that you are less dependent on typical development tools and services, i. e. to deploy to QA or production environments you need neither Internet access nor any compiler suites (for extension packages). To achieve the same with direct use of virtualenv and pip, you'd need to have an in-house PyPI repository accessible from production networks, and also release any extension packages as wheels pre-built for the target platform. Removing and updating an application is also much easier with Debian packages. To use dh-virtualenv, you just have to extend your existing application project with a debian subdirectory ‚Äì project meta-data like pip requirements and so on will be leveraged to build the final package, i. e. common tasks are delegated to the standard Python eco-system. Note that just like with any other form of omnibus packaging, you take over the responsibility to release security updates of the contained dependencies in a timely manner. How does it work? : dh-virtualenv is a debhelper plugin that extends the normal Debian tool chain for package building with the ability to create a Python virtualenv (an isolated Python environment), and then wrap that into the final Debian package. Depending on the details of the application, you often also have to provide some kind of configuration of the software itself, and possibly some means to run it as a service. This can be done in several ways: add a debian/¬´pkg¬ª. install descriptor to add configuration files to the Debian package. provide a Puppet recipe or Ansible playbook that deploys the package and integrates it into the system. embed (default) configuration into the application's Python package (via the include_package_data option of setuptools). All those can be combined, e. g. provide defaults via Python package data, and then add external configuration that only provides values specific to the concrete host installation. A real-world example is the devpi supervisor ERB template that serves both the purpose of passing configuration to the application process (via command line options), and also starting and controlling that process (i. e. handle demonization and automatic startup on boot). Installing the build tools : Unsurprisingly, you need to install dh-virtualenv to use it. Since it is architecture independant, you can choose to use a recent release offered by Debian sid whatever your build platform is. If this is your first time to build a Debian package, you also need to add the basic tools for that: sudo apt-get install build-essential debhelper devscripts equivsFinally, to take advantage of the available template for easily adding an inital debian directory, install the cookiecutter tool. Note that you can opt to build packages in a Docker container instead, with only Docker as a requirement on your build host. Packaging an example project : To add the necessary debian directory with minimal effort, you can use thedh-virtualenv-moldcookiecutter. The following commands basically repeat what the integration test script of that project does, namely instantiate a Python project and then add debianization on top of it. To provide common defaults to cookiecutter, it makes sense to have a ~/. cookiecutterrc file similar to the one I use. Let's first create a sample project: mkdir -p ~/tmp/dh-venv-blogcd ~/tmp/dh-venv-blogcookiecutter --no-input \  &quot;https://github. com/borntyping/cookiecutter-pypackage-minimal. git&quot;cd cookiecutter_pypackage_minimal/python3 setup. py buildYou can of course also use one of your own, then just check that out instead. Next, we add the debian directory: cookiecutter --no-input \  &quot;https://github. com/Springerle/dh-virtualenv-mold. git&quot;dch -r &quot;&quot; # insert proper date &amp; distroUsing --no-input causes the template's defaults to be accepted ‚Äì it avoids answering all the template's prompts. After all, this is just a demo not requiring sensible inputs. Take the time to have a look at what's in the debian directory. You're now able to build the package and if that succeeds, print the contained meta data: dpkg-buildpackage -uc -us -bdpkg-deb -I . . /pyvenv-foobar_*. debThe last command should show you something like this: Package: pyvenv-foobarVersion: 0. 1. 0Architecture: amd64Maintainer: J√ºrgen Hermann &lt;jh@web. de&gt;Installed-Size: 12877Pre-Depends: dpkg (&gt;= 1. 16. 1), python3, python3-venvSection: contrib/pythonPriority: extraHomepage: https://github. com/jschmoe/foobarDescription: A Python package and its dependencies packaged up as DEB in an isolated virtualenv. Finally, install the new package via dpkg -i, or upload it to a repository and use it from there with apt. Real-world examples : These are examples of dh-virtualenv packaging for non-trivial applications: devpinodeenvdebianized-sentryjupyterhubThe last two show how to integrate Python web applications into systemd, instead of using supervisor like the devpi example. The jupyterhub one also demonstrates the integration of a Python project with server-side Javascript (in a NodeJS environment). "
    }, {
    "id": 6,
    "url": "https://jhermann.github.io/python/documentation/2020/03/25/sphinx_ext_graphviz.html",
    "title": "Embedding Graphs Into Your Sphinx Documents",
    "body": "2020/03/25 -            The sphinx. ext. graphviz extension allows you to directly embed GraphViz ‚Äòdot language‚Äô graphs into your document files. They are then rendered to PNG or SVG images, which get added to your generated HTML documentation. Using SVG allows you to hot-link your nodes to any HTTP resource. Before use, you have to activate the extension with just a few changes to your docs/conf. py: extensions = [  # ‚Ä¶  &#39;sphinx. ext. graphviz&#39;,]# ‚Ä¶# -- GraphViz configuration ----------------------------------graphviz_output_format = &#39;svg&#39;This is an example for what you can then add to your documentation: As long as the nodes have a href attribute, the SVG rendering contains them and thus node labels become clickable hyperlinks. And here's the related markup that needs to be added to one of your . rst files: . . graphviz::  :name: sphinx. ext. graphviz  :caption: Sphinx and GraphViz Data Flow  :alt: How Sphinx and GraphViz Render the Final Document  :align: center   digraph &quot;sphinx-ext-graphviz&quot; {     size=&quot;6,4&quot;;     rankdir=&quot;LR&quot;;     graph [fontname=&quot;Verdana&quot;, fontsize=&quot;12&quot;];     node [fontname=&quot;Verdana&quot;, fontsize=&quot;12&quot;];     edge [fontname=&quot;Sans&quot;, fontsize=&quot;9&quot;];     sphinx [label=&quot;Sphinx&quot;, shape=&quot;component&quot;,          href=&quot;https://www. sphinx-doc. org/&quot;,          target=&quot;_blank&quot;];     dot [label=&quot;GraphViz&quot;, shape=&quot;component&quot;,       href=&quot;https://www. graphviz. org/&quot;,       target=&quot;_blank&quot;];     docs [label=&quot;Docs (. rst)&quot;, shape=&quot;folder&quot;,        fillcolor=green, style=filled];     svg_file [label=&quot;SVG Image&quot;, shape=&quot;note&quot;, fontcolor=white,         fillcolor=&quot;#3333ff&quot;, style=filled];     html_files [label=&quot;HTML Files&quot;, shape=&quot;folder&quot;,       fillcolor=yellow, style=filled];     docs -&gt; sphinx [label=&quot; parse &quot;];     sphinx -&gt; dot [label=&quot; call &quot;, style=dashed, arrowhead=none];     dot -&gt; svg_file [label=&quot; draw &quot;];     sphinx -&gt; html_files [label=&quot; render &quot;];     svg_file -&gt; html_files [style=dashed];   }For all this to work, you need the GraphViz suite of tools installed on the machine that renders the documentation. "
    }, {
    "id": 7,
    "url": "https://jhermann.github.io/python/testing/2020/03/21/tox_venv.html",
    "title": "Using Python3's ‚Äòvenv‚Äô with tox",
    "body": "2020/03/21 -            tox is a generic virtualenv management and test command line tool, especially useful for multi-environment testing. It has a plugin architecture, with plenty of both built-in and 3rd party extensions. This post assumes you are already familiar with tox and have a working configuration for it. If not, check out its documentation. In order to make tox use the built-in virtual environment venv of Python 3. 3+, there is a plugin named tox-venv that switches from using virtualenv to venv whenever it is available. Typically, venv is more robust when faced with ever-changing runtime environments and versions of related tooling (pip, setuptools, ‚Ä¶). To enable that plugin, add this to your tox. ini: [tox]requires = tox-venvThat merely triggers tox to check (on startup) that the plugin is installed. You still have to add it to your dev-requirements. txt or a similar file, so it gets installed together with tox. You can also install tox globally using dephell jail install tox tox-venv ‚Äì see the related post in this blog for details. The end result is this (call tox -v to see those messages): py38 create: ‚Ä¶/. tox/py38 ‚Ä¶/. tox$ /usr/bin/python3. 8 -m venv py38 &gt;‚Ä¶/log/py38-0. logAnd there you have it, no more virtualenv package needed. üéâ üéä "
    }, {
    "id": 8,
    "url": "https://jhermann.github.io/python/data-science/2020/03/17/data_pipes.html",
    "title": "Using R-style Data Pipelines in Notebooks",
    "body": "2020/03/17 -            Overview : This post shows how mutating data frames can be written more elegantly (and thus understood more easily) by using data pipelines. R users know this concept from the dplyr package, and Python offers a similar one named dfply. Setting the Stage : We start off with some global definitions‚Ä¶       import numpy as npimport pandas as pd    The sample data (about OS package deployments) is read into the raw_data dataframe from a CSV file.       raw_data = pd. read_csv(&quot;. . /assets/data/cmdb-packages. csv&quot;, sep=&#39;,&#39;)print(&#39;‚ôØ of Records: {}\n&#39;. format(len(raw_data)))for name in raw_data. columns[1:]:  if not name. startswith(&#39;Last &#39;):    print(name, &#39;=&#39;, list(sorted(set(raw_data[name]. fillna(&#39;&#39;)))))raw_data. head(3). transpose()  ‚ôØ of Records: 146Distribution = [&#39;Debian 8. 11&#39;, &#39;Debian 8. 6&#39;, &#39;Debian 8. 9&#39;, &#39;jessie&#39;]Architecture = [&#39;amd64&#39;]Environment = [&#39;&#39;, &#39;Canary&#39;, &#39;DEV&#39;, &#39;LIVE&#39;, &#39;QA&#39;]Team = [&#39;Automation&#39;, &#39;Big Data&#39;, &#39;Email&#39;, &#39;Ops App1&#39;, &#39;Ops Linux&#39;, &#39;Persistence&#39;, &#39;Platform&#39;]Installed version = [&#39;41. 15-2(amd64)&#39;, &#39;42. 28-2(amd64)&#39;, &#39;42. 44-1(amd64)&#39;, &#39;45. 11-1(amd64)&#39;, &#39;48. 33-1(amd64)&#39;]         0   1   2         CMDB_Id   274656589   153062618   282201163       Distribution   jessie   jessie   jessie       Architecture   amd64   amd64   amd64       Environment   DEV   DEV   LIVE       Team   Ops App1   Ops App1   Ops App1       Last seen   2019-02-18 11:43   2019-02-18 11:56   2019-02-18 12:04       Last modified   2019-02-18 11:43   2019-02-18 11:56   2019-02-18 12:04       Installed version   42. 28-2(amd64)   42. 28-2(amd64)   48. 33-1(amd64)           def map_distro(name):  &quot;&quot;&quot;Helper to create canonical OS names. &quot;&quot;&quot;  return (name. split(&#39;. &#39;, 1)[0]    . replace(&#39;Debian 7&#39;, &#39;wheezy&#39;)    . replace(&#39;Debian 8&#39;, &#39;jessie&#39;)    . replace(&#39;Debian 9&#39;, &#39;stretch&#39;)    . replace(&#39;Debian 10&#39;, &#39;buster&#39;)    . replace(&#39;squeeze&#39;, &#39;Squeeze [6]&#39;)    . replace(&#39;wheezy&#39;, &#39;Wheezy [7]&#39;)    . replace(&#39;jessie&#39;, &#39;Jessie [8]&#39;)    . replace(&#39;stretch&#39;, &#39;Stretch [9]&#39;)    . replace(&#39;buster&#39;, &#39;Buster [10]&#39;)  )    Data Cleaning With Pandas : This code cleans up the imported data using the Pandas API. To get sensible version statistics, we split off the auxiliary information in the version column (anything after -), leaving just the upstream part of the version string. The environment classifier is also cleaned up a little, and distributions are mapped to a canonical set of names. Some unused columns are dropped. Finally, a subset of unique version samples is selected.       data = raw_datadata = data. assign(Version=data[&#39;Installed version&#39;]. str. split(&#39;-&#39;, 1, expand=True)[0])data = data. assign(Environment=data. Environment. fillna(&#39;UNDEFINED&#39;). str. upper())data = data. assign(Distribution=data. Distribution. apply(map_distro))data = data. drop(columns=[&#39;CMDB_Id&#39;, &#39;Last seen&#39;, &#39;Last modified&#39;, &#39;Installed version&#39;])data = data. drop_duplicates(subset=&#39;Version&#39;, keep=&#39;first&#39;)data. transpose()           0   2   26   45   62         Distribution   Jessie [8]   Jessie [8]   Jessie [8]   Jessie [8]   Jessie [8]       Architecture   amd64   amd64   amd64   amd64   amd64       Environment   DEV   LIVE   LIVE   UNDEFINED   DEV       Team   Ops App1   Ops App1   Platform   Email   Platform       Version   42. 28   48. 33   41. 15   45. 11   42. 44     Data Cleaning With Pipelines : This does the exact same processing as the code above, but is arguably more readable and maintained more easily: It has less boilerplate, and makes the use of pipelined processing transparent. Each step clearly states what it does to the data. When steps are copied into other pipelines, the X placeholder ensures you use the data of this pipeline (the code is more DRY). .       from dfply import *piped = (raw_data  &gt;&gt; mutate(Version=X[&#39;Installed version&#39;]. str. split(&#39;-&#39;, 1, expand=True)[0])  &gt;&gt; mutate(Environment=X. Environment. fillna(&#39;UNDEFINED&#39;). str. upper())  &gt;&gt; mutate(Distribution=X. Distribution. apply(map_distro))  &gt;&gt; drop(X. CMDB_Id, X[&#39;Last seen&#39;], X[&#39;Last modified&#39;], X[&#39;Installed version&#39;])  &gt;&gt; distinct(X. Version))piped. transpose()           0   2   26   45   62         Distribution   Jessie [8]   Jessie [8]   Jessie [8]   Jessie [8]   Jessie [8]       Architecture   amd64   amd64   amd64   amd64   amd64       Environment   DEV   LIVE   LIVE   UNDEFINED   DEV       Team   Ops App1   Ops App1   Platform   Email   Platform       Version   42. 28   48. 33   41. 15   45. 11   42. 44     The result is identical to the pure Pandas code, as expected. To learn more about dfply, read the dplyr-style Data Manipulation with Pipes in Python blog post, which has more examples. Reference Links : dfply : dplyr-style Data Manipulation with Pipes in Python ‚Äì Towards Data Sciencekieferk/dfply: dplyr-style piping operations for Pandas dataframesAlternatives : has2k1/plydata: A grammar for data manipulation in Pythonshaypal5/pdpipe: Easy pipelines for Pandas dataframes"
    }, {
    "id": 9,
    "url": "https://jhermann.github.io/linux/deployment/2020/03/14/fpm_effing_package_managers.html",
    "title": "Packaging Software with ‚Äòfpm‚Äô",
    "body": "2020/03/14 -            What it does : Basically, ‚Äòfpm‚Äô allows you to deploy any software via OS packages,from an installation tree on disk or from already built artifacts specific tothe chosen implementation language. The main advantage over native tooling is you do not need to knowabout every minute detail of the involved commands and metafile formatsfor every platform. In case of Debian, that is at least the ‚Äúcontrol‚Äù and ‚Äúrules‚Äù files,and tools like ‚Äúbuildpackage‚Äù and ‚Äúdebhelpers‚Äù. How to install it : ‚Äòfpm‚Äô is written in Ruby and can thus be installed via gem install. But you can also run it with JRuby in a JVM,that you might happen to already have on a host anyway. Using JRuby spares you the headache of juggling multiple Ruby versionsand isolating gems against other Ruby applications. If you package it up that way, it also can be installed on any Linux releasebecause you only need a Java8 JRE installed to run it‚Äì no native code involved. You can use the fpm. sh script in thepriscilla project on GitHubto package ‚Äòfpm‚Äô with itself. As written, it works for Debian derivatives,but should be adaptable to other distros with a few changes(remember, ‚Äòfpm‚Äô makes that easy). If you call . /fpm. sh pkg, the package contents is created inbuild as a staging area, and when everything is ready, fpm is called from within that staging area to create the final package: build/opt_tools_fpm/opt/tools/fpm/bin/fpm \  -s dir -t deb -n opt-tools-fpm -v 1. 11. 0 \  --iteration 1 --category tools \  --deb-user root --deb-group root \  -m ' Juergen Hermann  &lt;jh@web. de&gt;' \  --license 'See contained license, or homepage' \  --vendor github. com/jhermann/priscilla \  --description 'fpm helps you build packages quickly and easily' \  --url http://fpm. readthedocs. io/ \  --workdir $PWD/build/opt_tools_fpm/tmp \  -a all -d 'openjdk-8-jre|‚Ä¶|java8-runtime-headless' \  opt usrYes, this is a mouthful, but still shorter than a control or . spec file,and easily adapted to other package managers. This created a DEB file . /build/opt_tools_fpm/opt-tools-fpm_1. 11. 0-1_all. deb, with this metadata: new debian package, version 2. 0. size 23763108 bytes: control archive= 30446 bytes.   462 bytes,  12 lines   control        119039 bytes, 1118 lines   md5sums        Package: opt-tools-fpm Version: 1. 11. 0-1 License: See contained license, or homepage Vendor: github. com/jhermann/priscilla Architecture: all Maintainer:  Juergen Hermann  &lt;jh@web. de&gt; Installed-Size: 27046 Depends: openjdk-8-jre|zulu8|‚Ä¶|java8-runtime-headless Section: tools Priority: extra Homepage: http://fpm. readthedocs. io/ Description: fpm helps you build packages quickly and easilyThe major part of files is installed into /opt/tools/fpm,but a symlink at /usr/bin/fpm makes the command available on the path. Building fpm 1. 11. 0 that way was tested on Ubuntu Bionic usingopenjdk-8-jre-headless 8u242. How to use it : There are a lot of source and target types available in fpm(dir, gem, deb, npm, rpm, tar, cpan, pear, empty, puppet, python, osxpkg, solaris, p5p, pkgin, freebsd, apk, snap, pleaserun, zip, virtualenv, pacman, sh),this example converts a Python workdir into a DEB package file. The rudiments project serves as the example here, but you can use any pure Python project built with setuptools. You have to clone the project and then call fpm like so: ( deactivate 2&gt;/dev/null; py=/usr/bin/python3; \ fpm -s python -t deb --category python \  --python-bin $py \  --python-pip  $py -m pip  \  --python-package-name-prefix  $(basename $py)  \  --python-obey-requirements-txt \  --python-install-data  /usr/local/share/$(basename $py)/$($py . /setup. py --name)  \  -m  \ $($py . /setup. py --author)\  &lt;$($py . /setup. py --author-email)&gt;  \  --vendor  $($py . /setup. py --url | cut -f3-4 -d/)  \  --force $PWD/setup. py )The --force option overwrites an existing package file, so you can call the command multiple times without an error. If you inspect the built package with dpkg-deb -I python3-rudiments_*_all. deb,this is the output: new Debian package, version 2. 0. size 31062 bytes: control archive=1417 bytes.   366 bytes,  12 lines   control         3789 bytes,  30 lines   md5sums        Package: python3-rudiments Version: 0. 3. 1 License: Apache 2. 0 Vendor: github. com/jhermann Architecture: all Maintainer:  J√ºrgen Hermann  &lt;jh@web. de&gt; Installed-Size: 79 Depends: python3-requests (&gt;= 2. 6) Section: python Priority: extra Homepage: https://github. com/jhermann/rudiments Description: Rudiments ‚Äì Fundamental elements for any Python project. The package's content is placed into these directories: /usr/local/lib/python3. 6/usr/local/share/python3/rudiments/usr/share/doc/python3-rudimentsYou can install it using dpkg -i ‚Ä¶ and then do a quick import test with this command: python3 -c  import rudiments; print(rudiments) Where to go from here : Read more about ‚Äòfpm‚Äô on its github wiki, or watch this slide deck. Credits: Package icon by Breathe Icon Team: Sebastian Porta, Cory Kontros, Andrew Starr-Bochicchio "
    }, {
    "id": 10,
    "url": "https://jhermann.github.io/python/deployment/2020/03/08/ship_libs_with_shiv.html",
    "title": "Bundling Python Dependencies in a ZIP Archive",
    "body": "2020/03/08 -            The Basic Idea : If you have a set of Python scripts that are all using the same set of required packages, you can distribute those dependencies in the form of a zipapp, i. e. in a single executable file. See Building Zipapps (PEP 441) for details if you're new to the concept of zipped Python application bundles Unlike shipping a script in a virtualenv built within a single project, you can have a project for the base libraries and other projects for the scripts, including scripts written by end users who are just using your dependencies. You can also deploy any PyPI package that way, with a simple call of shiv, as shown in the next section using Pandas. A Practical Example : The following example uses the well-known Pandas data science library, but this works for any project built with setuptools or any other build tool creating Python packages that declare their requirements. So, to create your base library release artifact, install and call shiv like this: python3. 8 -m pip install --user shivpython3. 8 -m shiv -p '/usr/bin/python3. 8 -IS' \         -o ~/bin/_lib-pandas pandas==1. 0. 1Do this in a virtualenv and leave out the --user option if you want to keep your account's home directory clean. Note that we do not provide an entry point here, which means this zipapp drops into the given Python interpreter and is thus usable as an interpreter, with the contained packages available for import. Now we can exploit this to write a script using the zipapp as its interpreter: cat &gt;script &lt;&lt;'EOF'#! /usr/bin/env _lib-pandasimport reimport sysfrom pathlib import Pathimport pandas as pdprint('Using Pandas from',   Path(pd. __file__). parent. relative_to(Path. home()),   '\n\nPython path:')df = pd. DataFrame(sys. path, columns=['Path'])df. Path = df. Path. str. replace(f'^{ re. escape(str(Path. home())) }/', '~/')print(df)EOFchmod +x script. /scriptCalling the script produces the following output: Using Pandas from . shiv/_lib-pandas_23b2‚Ä¶d2/site-packages/pandas Python path:                        Path0                 ~/bin/_lib-pandas1               /usr/lib/python38. zip2                 /usr/lib/python3. 83           /usr/lib/python3. 8/lib-dynload4 ~/. shiv/_lib-pandas_23b2bb7d64c26139950435a64d. . . If you're familiar with Pandas, you'll instantly recognize the Python path output as coming from a Pandas data frame. üéâ This first execution is a bit slow on startup, because the cache directory you see at the end of the Python path has to be populated first. shiv's boot-strapping code unpacks extension packages containing native code into the file system, so the OS can load them. The underscore prefix in the zipapp name indicates this is not a command humans would normally use. Alternatively and especially in production you can deploy into e. g. /usr/local/lib/python3. 8/ and then use an absolute path instead of an env call as the script's interpreter. "
    }, {
    "id": 11,
    "url": "https://jhermann.github.io/python/deployment/2020/03/03/install_tools_with_dephell.html",
    "title": "Installing CLI Tools Using ‚Äòdephell‚Äô",
    "body": "2020/03/03 -            Introduction : ‚Äòdephell‚Äô is a useful add-on tool for project and venv management that works with existing standard tooling, instead of doing a bad replacement job like so many others. This post takes a look at how it can take over from pipsi (Python Script Installer, which is unmaintained) to manage isolated tool installations by providing each tool with its own virtual environment. Installation : Dephell is installed via a Python installer script into its own venv (compatible to what dephell itself creates as a so-called ‚Äòjail‚Äô). curl -L dephell. org/install | python3dephell needs at least Python 3. 6, which is the default on Ubuntu Bionic, so it just works‚Ñ¢ there. On Xenial, you need to install 3. 6+ from the Deadsnakes PPA first, and pipe the installer script into e. g. python3. 8. The only locations touched by the installer on a Posix host are ~/. local/bin/ and ~/. local/share/dephell/venvs/. Going into &#8216;jail&#8217; : As already mentioned, this post will take a deeper look into the dephell jail sub-command for venv management. Unlike pipsi, the former go-to tool for that purpose, it is maintained, supports full life-cycle management (i. e. it has a way to remove tool installations), and also supports projects that have several console entry points (i. e. expose more than one command). I also like it a lot more than pipx, which has a similar feature profile when compared to just dephell's jail sub-command, but YMMV. As a first example, to get rid of dephell again, just remove it using itself: dephell jail remove dephellNote that doing so leaves anything installed via dephell untouched (i. e. other jails still work), and reinstalling allows to manage those again. Adding more tools is done using jail install: dephell jail install shivshiv --versionMake sure that ~/. local/bin is in your PATH, which is not always the case on older GNU/Linux releases. You can easily list what you have installed: $ dephell jail list{  dephell : [   dephell  ],  shiv : [   shiv-info ,   shiv  ]}As you can see, the output is JSON by default and lists all installed tools with their possibly multiple entry points. You can add the --table option to get output more suited for humans. To see more details about a single venv, use jail show: $ dephell jail show dephell{  entrypoints : [   dephell  ],  name :  dephell ,  path :  /home/jhe/. local/share/dephell/venvs/dephell ,  size : {   lib :  43. 21Mb ,   total :  56. 78Mb  },  version :  0. 8. 1 }Finally, there is a jail try command to give new tools a quick spin in a temporary environment, without leaving any trace of it on your machine. $ dephell jail try --command  pip --version  pip‚Ä¶INFO running. . . pip 20. 0. 2 from /tmp/tmpnm5gvieo/lib/python3. 6/site-packages/pip (python 3. 6)Beyond &#8216;jail&#8217; : Besides jail, there are lots of other sub-commands for dependency management, handling docker images, creating common Python software project files, managing and vendoring your project's dependencies, and handling of project-specific venvs. See the full DepHell documentation for details on that. "
    }, {
    "id": 12,
    "url": "https://jhermann.github.io/python/deployment/2020/02/29/python_zippapps_on_windows.html",
    "title": "Enabling Easy Zipapp Installs on Windows",
    "body": "2020/02/29 -            Zipapps in a Nutshell : Zipapps are a way to distribute Python applicationsand all of their dependencies in a single binary file. This is comparable to statically linked golang apps or Java's ‚Äòexecutable JARs‚Äô. Their main advantage is that distributing and installing them is quite simple. Running Python code directly from ZIP archives is nothing new, PEP 273 made its debut in 2001, as part of Python 2. 3 in the form of the zipimport module. PEP 441 builds on this and describes mechanisms to bundle full applications into a single ZIP file that can be made executable. It was approved in 2015 and a first implementation appeared in Python 3. 5 via the zipapp module. See the PEP for details on how making a ZIP into an executable file works, but basically on POSIX systems the Python interpreter is called in a ‚Äòbang path‚Äô that is followed by the ZIP archive. The interpreter recognizes the ‚Äòscript‚Äô is a whole application archive and acts accordingly. On Windows, zipapps MUST carry the . pyz extension which is bound to the py wrapper command, which in turn looks at the bang path and calls a matching Python interpreter from the installed set. To display the bang path of a zipapp, use this command: python3 -m zipapp --info foo. pyzIf you want to change the requested Python version to one that is actually installed or that you prefer, change the bang path as part of the installation process: python3 -m zipapp -p '/usr/bin/env python3. 8' -o ~/bin/foo foo. pyzThis can also be done on an ad-hoc basis, by explicitly calling the desired interpreter: python3. 8 foo. pyz ‚Ä¶ # POSIXpy -3. 8 foo. pyz ‚Ä¶  # WindowsWell-known tools to build new zipapps, outside of the Python core, are pex (Twitter) and shiv (LinkedIn). See their documentation for details on bundling your own applications. Setting Up Windows 10 for Zipapps : On Windows, because there is no ‚Äò+x‚Äô flag, things are a bit more complicated than on POSIX. Zipapps MUST have a . pyz extension,for which the py launcher is registered as the default application. The net effect is that such files become executable and are handed over to the launcherif you add a few environment settings to your machine. In the user-specific environment settings, add a new PATHEXT variable(or extend an existing one), with the value %PATHEXT%;. PYZ. Also edit the PATH one and add a new %LOCALAPPDATA%\bin entry. Save everything (click ‚ÄúOK‚Äù), open a new command window, and verifythe changes with echo %PATHEXT% &amp; echo %PATH%Create the new bin directory by calling md %LOCALAPPDATA%\bin. Now you can place a zipapp file like foo. pyz in that directory,and it is immediately callable as foo. To get such a test subject, you can build shiv with itself: git clone https://github. com/linkedin/shiv. gitcd shivpy -3 -m venv --prompt shiv venvvenv\Scripts\activate. batpython -m pip install -e . shiv -e shiv. cli:main -o %LOCALAPPDATA%\bin\shiv. pyz . deactivateshiv --versionVariations : If that makes more sense to you, you can change the system-widevariables instead of the user-specific ones, and choose paths that areglobal for all users (like C:\usr\bin or similar). To make zipapps available network-wide, you can use %APPDATA% to store the zipapps,so you only have to maintain them once in case you regularlywork on several machines in the same network. Just make sure the same version of Python is used everywhere then. "
    }, {
    "id": 13,
    "url": "https://jhermann.github.io/linux/know-how/2020/02/28/env_with_arguments.html",
    "title": "Shell Scripts: env-shebang with Arguments",
    "body": "2020/02/28 -            The Problem : There is an old annoyance that, if you use env in a bang path to search the script interpreter in the shell's path, you cannot pass any arguments to it. Instead, all the text after the call to env is passed as one single argument, and env tries to find this as the executable to invoke, which fails of course when arguments are present. env is not the culprit here, but the very definition of how a bang path works (quoted from the bash manpage): If the program is a file beginning with #!, the remainder of the first line specifies an interpreter for the program. The shell executes the specified interpreter on operating systems that do not handle this executable format themselves. The arguments to the interpreter consist of a single optional argument following the interpreter name on the first line‚Ä¶ (emphasis mine) So what env gets to see in its argv array when you write something like #! /usr/bin/env python3 -I -S is ['/usr/bin/env', 'python3 -I -S']. And there is no python3 -I -S anywhere to be found that could interpret your script. üòû The Solution : The env command in coreutils 8. 30 solves this (i. e. Debian Buster only so far, Ubuntu Bionic still has 8. 28). The relevant change is introducing a split option (-S), designed to handle that special case of getting all arguments mushed together into one. In the example below, we want to pass the -I -S options to Python on startup. They increase security of a script, by reducing the possible ways an attacker can insert their malicious code into your runtime environment, as you can see from the help text: -I   : isolate Python from the user's environment (implies -E and -s)-E   : ignore PYTHON* environment variables (such as PYTHONPATH)-s   : don't add user site directory to sys. path; also PYTHONNOUSERSITE-S   : don't imply 'import site' on initializationYou can try the following yourself using docker run --rm -it --entrypoint /bin/bash python:3-slim-buster: $ cat &gt;isolated &lt;&lt;&#39;. &#39;#!/usr/bin/env -S python3 -I -Simport sysprint(&#39;\n&#39;. join(sys. path)). $ chmod +x isolated$ . /isolated/usr/local/lib/python38. zip/usr/local/lib/python3. 8/usr/local/lib/python3. 8/lib-dynloadNormally, the Python path would include both the current working directory (/ in this case) as well as site packages (/usr/local/lib/python3. 8/site-packages). However, we prevented their inclusion as a source of unanticipated code ‚Äì and you can be a happy cat again. üòª "
    }, {
    "id": 14,
    "url": "https://jhermann.github.io/tools/automation/2020/02/27/autoenv.html",
    "title": "Simplify Your Developer Life with `autoenv`",
    "body": "2020/02/27 -           When you work a lot with Python venvs,and thus have a lot of them sprinkled over your home directory,then remembering to activate the right one can be a source of problems,and is tedious at best. But there is a solution to automate that chore‚Äì that's what we have those boxes full of electronics for, after all. Meet autoenv : Consider this shell session and especially watch what happens to the prompt. jhe@workstation:~$ which python/usr/bin/pythonjhe@workstation:~$ cd src/github/rituals/(rituals)jhe@workstation:~/src/github/rituals$ which python/home/jhe/src/github/rituals/. venv/rituals/bin/python The magic wand was originally crafted by Kenneth Reitz,and can be found on GitHub. Installing autoenv : To get a working installation, the easiest way is to directly use a git checkout as follows: mkdir -p ~/. localtest -d ~/. local/autoenv \  || git clone  https://github. com/kennethreitz/autoenv. git  \         ~/. local/autoenvecho &gt;&gt;~/. bash_aliases  . ~/. local/autoenv/activate. sh . ~/. local/autoenv/activate. shThat's all there is to it. Now you just have to add a . env file to your project,like in this example. Security Considerations : If you're afraid that the cd command is wrapped by a bash function,the following shows that you need not fear,since that function isn't exported to any scripts you run. (rituals)jhe@workstation:~/src/github/rituals$ bash &lt;&lt;&lt; pwd; \  cd $PWD/. . /time-tunnel; pwd; which python /home/jhe/src/github/rituals/home/jhe/src/github/time-tunnel/home/jhe/src/github/rituals/. venv/rituals/bin/pythonThis pretty much restricts the modified cd to interactive use. Sub-shells behave differently, again that's what you'd expect working at the prompt. (rituals)jhe@workstation:~/src/github/rituals$ ( pwd; \  cd $PWD/. . /time-tunnel; pwd; which python )/home/jhe/src/github/rituals/home/jhe/src/github/time-tunnel/home/jhe/src/github/time-tunnel/. venv/time-tunnel/bin/pythonIf at any time you need the original command on the prompt, just use command cd ‚Ä¶ or builtin cd ‚Ä¶. Also, nobody can inject code into your shell just so, see what happens if we stumble overa new or modified . env file the first time: (rituals)jhe@workstation:~/src/github/rituals$ cd . (rituals)jhe@workstation:~/src/github/rituals$ echo &gt;&gt;. env(rituals)jhe@workstation:~/src/github/rituals$ cd . autoenv:autoenv: WARNING:autoenv: This is the first time you are about to source /home/jhe/src/github/rituals/. env:autoenv:autoenv:   --- (begin contents) ---------------------------------------autoenv:   # autoenv script (https://github. com/kennethreitz/autoenv)autoenv:   test \! -f . venv/$(basename $(pwd))/bin/activate || . . venv/$(basename $(pwd))/bin/activateautoenv:   autoenv:autoenv:   --- (end contents) -----------------------------------------autoenv:autoenv: Are you sure you want to allow this? (y/N) yNow you have all the information to decide whether this is something you'd like to use or not. I do, but YMMV. ü§î "
    }, {
    "id": 15,
    "url": "https://jhermann.github.io/python/deployment/2020/02/26/deadsnakes_on_debian.html",
    "title": "(Dead) Snakes on a‚Ä¶ Debian System",
    "body": "2020/02/26 -            The Deadsnakes PPA project originally built older Python releases for Ubuntu, so you could e. g. run unit tests on a new release using a Python version found on older releases (i. e. the ‚Äòdead‚Äô snakes). Nowadays, the project also builds newer Python versions ahead of what a certain release offers as its default. The packages contain the minor Python version in their name (e. g. python3. 6) and can thus be installed concurrently to the default python3 ones. Originally based on the Debian source packages, they can also be used on Debian and not just on Ubuntu. The build script and Dockerfile found here build packages for some Debian releases in their related Docker base images. Based on this, Python 3. 6 can be installed for all of Stretch, Buster, and Xenial, as a set of the usual core Python packages (python3. 6, python3. 6-venv, python3. 6-dev, ‚Ä¶). Note that Bionic comes with 3. 6 as a default. The same goes for Python 3. 7, with Buster having it as a default. Python 3. 8 is an add-on for all the (old-)stable releases (as of Feb 2020). Using this version makes the most sense to me, unless you have special needs forcing you to go to 3. 7 or 3. 6. Being an add-on everywhere ensures a similar experience regarding any quirks you encounter, and it is (right now) the newest stable version of Python. It also fits best what you get when using Docker's python:3-slim. "
    }, {
    "id": 16,
    "url": "https://jhermann.github.io/devops/continuous-delivery/2020/02/25/continuous-delivery.html",
    "title": "Continuous Delivery Explained",
    "body": "2020/02/25 -            I wrote this back in September 2014 and never published it, but since it's an introductory piece it stands its ground, so let this serve as an initial post‚Ä¶ CD in a Nutshell : A typical mission statement for Continuous Delivery is this‚Ä¶ Our highest priority is to satisfy the customer,through early and continuous delivery of valuable software. Continuous Delivery strives to improve the process of software delivery, by applying Continuous Deployment paired with automated testing and Continuous Integration. The goal is creating software developed to a high standard and easily packaged and deployed to test environments, resulting in the ability to rapidly, reliably and repeatedly push out enhancements and bug fixes to customers in small increments, at low risk and with minimal manual overhead. CD is effective because it facilitates an explorative approach by providing real, valuable measurements of the output of the process, and feeding those results back into the process. It's the next logical step after applying Agile principles to development, by expanding the scope to the whole software life-cycle and all involved parties, from inception to going live and then maintaining the product for a substantial amount of time in fast-paced iterations. Some More Details : Continuous Delivery means that your software is production-ready from day one of your project (even when it's not ‚Äúfeature complete‚Äù), and that you can release to users on demand at the push of a button. There are several practices and patterns that enable this, but the foundation is formed in particular by excellent configuration management, continuous integration, and comprehensive automated testing at all levels. The key pattern is the deployment pipeline, which is effectively the extension of continuous integration out to production, whereby every check-in produces a release candidate which is assessed for its fitness to be released to production through a series of automated and then manual tests. In order to be able to perform these validations against every build, your regression tests must be automated ‚Äî both at the unit and acceptance level. Humans then perform tasks such as exploratory testing, usability testing, and showcases as later validations against builds that have already passed the automated tests. Builds can be deployed automatically on demand to testing, staging and production environments by the people authorized to do so ‚Äî note that this means deployments are triggered by humans and performed by machines. Through these practices, teams can get fast feedback on whether the software being delivered is useful, reduce the risk of release, and achieve a much more predictable, reliable process for software delivery. The backbone of CD is a culture in which everybody, if somehow involved in the delivery process, collaborates throughout the life-cycle of the product ‚Äî developers, testers, infrastructure, operators, DBAs, managers, and customers alike. Where to Go From Here? : Here are some resources for diving deeper into the topic: Jez Humble's Blog ¬∑ Continuous DeliveryCD Foundation ‚Äì A Neutral Home for the Next Generation of Continuous Delivery Collaboration. IT Revolution DevOps BlogDevops Weekly Mailing List (by @garethr)Team Topologiesüëç Credits: Devops-toolchain "
    }, {
    "id": 17,
    "url": "https://jhermann.github.io/how-to/know-how/2020/02/22/talks+presentations.html",
    "title": "Talks & Presentations",
    "body": "2020/02/22 -            Slides : On Speakerdeck: JupyterHub and Jupyter Notebook ‚Äì A View Under the Hood (PyData S√ºdwest Meetup KA ¬∑ 2018-04-24)Document the Data ‚Äì Creating Reports Using Docs Tooling (PyData S√ºdwest Meetup KA ¬∑ 2018-06-13)DevOps Tool Bazaar ‚Äì dh-virtualenv, fpm, sentry. io (DevOps Karlsruhe Meetup ¬∑ 2018-02-20)Videos : "
    }, {
    "id": 18,
    "url": "https://jhermann.github.io/misc/development/2020/02/21/projects_guided_tour.html",
    "title": "A Guided Tour of My Projects",
    "body": "2020/02/21 -           üöß This article is work in progress, and is updated regularly with new content.  For Python Developers : The Springerle GitHub organization is a collection of cookiecutter project templates, with templates for these types of project: single-file scripts (py-minimal-script),fully equipped packages and applications (py-generic-project),DEB packaging of existing projects (dh-virtualenv-mold ¬∑ debianized-pypi-mold),and more. Fundamental elements for any Python project, like configuration handling, are in the rudiments package It is used for runtime support in the Springerle templates mentioned above. On the project automation side of things, rituals is a library of Invoke tasks that are needed again and again. The Springerle templates use it to add management tasks that are updateable via pip, independently of the template. dependency-check-py is a shim to easily install OWASP dependency-check-cli into Python projects, by adding it to your requirements. The collection of Jupyter notebooks in whats-new-in-python3 summarize the ‚ÄúWhat's new in Python3?‚Äù documentation, with live code in the notebooks. Right now, it is unfinished and work in progress. Data Science &amp; Jupyter : jupyter-by-example has learning resources and practical tips on how to use Jupyter notebooks for fun &amp; profit. The Today I Learned about Data Science‚Ä¶ wiki contains similar information, with an extended scope (beyond Jupyter). See also the debianized-jupyterhub project further below. Software Design &amp; Architecture : In c4-notation I collect technical resources about using the C4 model for visualizing software architecture. This is so far rather small and unfinished. See also my Diagrams as Code wiki page for similar resources. Debian Packages : I'm a contributor to dh-virtualenv, and I use it for all my Debian packaging needs. Especially when it comes to deploying applications with lots of dependencies, or services needing tight integration with the host (i. e. systemd units). For pure command line tools, pex and shiv can be a better alternative. There are several projects using dh-virtualenv as the basic packaging tool: 1and1/debianized-jupyterhub packages JupyterHub, a multi-user server for Jupyter notebooks, It also comes with a Python3 kernel, populated with an extensive data science stack. 1and1/debianized-sentry puts all sentry. io 9. x services into one package using systemd as a supervisor, you just need to add a PostgreSQL databasse. devpi-enterprisey/debianized-devpi allows easy deployment of the devpi package repository and proxy. This doesn't get updated that often, it basically chugs along silently on my workstations, speeding up virtualenv creation and allowing off-line work. Note that these are typically built in a Docker container. See the For Python Developers section on how to easily roll your own projects of this type. The dput-webdav plugin then allows you to use dput to comfortably upload created packages to a WebDAV repository like Artifactory (BinTray). Also check out (Dead) Snakes on a‚Ä¶ Debian System for being able to install newer Python versions on all major Debian-like releases. Docker &amp; Kubernetes : dockyard offers basic Dockerfile templates and other Docker build helpers. It contains some experiments regarding Python base images, shows how to build Debian packages within a container for repeatable builds, and comes with extensive documentation also showing how to optimize your Dockerfiles. Miscellaneous : The README of awesome-python-talks is an opinionated list of videos related to Python, with a focus on training and gaining hands-on experience. The awesome-tech-talks repository is very similar, but about software development and general IT topics. confluencer contains a CLI tool that automates common Atlassian Confluence maintenance tasks and content publishing. There's no place like home‚Ä¶ ruby-slippers is my dotfiles repository (and I found ‚Äúdotfiles‚Äù way too boring as a name). It also includes setup scripts for installing a bunch of developer tools and packages. "
    }, {
    "id": 19,
    "url": "https://jhermann.github.io/how-to/fastpages/2020/02/20/fastpages-pitfalls.html",
    "title": "fastpages: Pitfalls, Tips & Tricks",
    "body": "2020/02/20 -           üÜï üìù This article is updated regularly with new information as it is discovered. Introduction : fastpages will automatically convert Jupyter Notebooks saved into the _notebooks directory as blog posts! You must save your notebook with the naming convention YYYY-MM-DD-*. ipynb. Otherwise the file's modification time is used, but that is not something you should rely on. See Writing Blog Posts With Jupyter for more details on special markup features. Things to Consider : Isolate the title, subtitle, and metadata into their own cell (the first one). Always quote your title and sub-title. Also quote any categories that are not purely alpha-numerical-with-hyphens. Add author information to your pages, so the Atom feed has it ‚Äì Adding authors to your Jekyll site. Categories are hierarchical and become part of the final URL ‚Äì so give them a reasonable order (from generic to specific), have a consistent scheme for the site as a whole, and do not change them after publishing, otherwise you'll break people's links. For dual-use images (referenced in notebook contents and metadata), place the image in the _notebooks folder (or a sub-folder), use the relative path there in your notebook, and prefix that path for the image: attribute with images/copied_from_nb/. Categories must be listed in a bracketed list ‚Äì you cannot make it multi-line. Do not use today's date in filenames if you plan to publish soonish ‚Äì timezone differences to the build machines will possibly make your article disappear from the generated index. (fixed in template)This is a bit subjective, but I recommend to get yourself a local installation of JupyterHub and use that exclusively as your ‚Äúfastpages IDE‚Äù ‚Äì that means just ignore plain markdown files and do everything with notebooks. How-Tos : Updating the fastpages Template : To get changes from upstream, check out your own copy of the template project. To catch up for the first time, find the Initial commit in your blog repository, then make sure there are no commits after the time of that commit in the template. If there are, find the last commit before that date (the last one you already have), and use its SHA for a range ¬´sha¬ª. . HEAD in the update procedure that follows, starting with the git diff. Now to catch up, when you git pull --ff-only in the template, there is a line starting with Updating and a SHA range. Call git diff ¬´sha-range¬ª &gt;/tmp/fastpages. patch. Change to your blog repository workdir (make sure you have a clean workdir with no pending changes) and apply that patch: patch -N -p1 &lt;/tmp/fastpages. patch. Look out for patch rejects and resolve any conflicts. Finally commit the template changes, ideally mentioning the SHA range in the commit message for record-keeping (example). Instead of using patch, you can also open a branch for upstream changes, git pull --ff-only upstream/master any updates into that branch, and then git merge that branch into master. This way, git does all the house-keeping and memorizing of SHAs. Troubleshooting : Article is not in the Index : See above hint regarding dates in the (near) future, and timezone differences between localhost and the cloud. Index Entry is Missing Most Fields : When an entry appears mostly empty, there is some metadata problem: missing quotes, improper YAML syntax, etc. Backtrack your changes in the git history, and never change metadata in bulk, so you can isolate the problem. And wait for a successful build after each change. "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')
    this.metadataWhitelist = ['position']

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}